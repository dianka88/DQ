{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.1.3\n",
      "Uninstalling numpy-2.1.3:\n",
      "  Successfully uninstalled numpy-2.1.3\n",
      "Found existing installation: pandas 1.3.5\n",
      "Not uninstalling pandas at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "Can't uninstall 'pandas'. No files were found to uninstall.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install --no-cache-dir numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \n",
       "0                 138          20      7  telecom industry  2010    472  \n",
       "1                  34           6      2             stock  2010    578  \n",
       "2                  78          11      0          pharmacy  2010    673  \n",
       "3                  80          13      0          pharmacy  2010    673  \n",
       "4                 137          21      7  telecom industry  2010    472  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv (\"/home/pedige1/DQ-2/train_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            16902\n",
       "id                    16902\n",
       "date                  16902\n",
       "news                  16902\n",
       "label                 16902\n",
       "pct_change            16902\n",
       "text_length           16902\n",
       "textlemma             16902\n",
       "textlengthforlemma    16902\n",
       "word_count            16902\n",
       "topic                 16902\n",
       "topic_name            16902\n",
       "year                  16902\n",
       "count                 16902\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16902 entries, 0 to 16901\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          16902 non-null  int64  \n",
      " 1   id                  16902 non-null  object \n",
      " 2   date                16902 non-null  object \n",
      " 3   news                16902 non-null  object \n",
      " 4   label               16902 non-null  object \n",
      " 5   pct_change          16902 non-null  float64\n",
      " 6   text_length         16902 non-null  int64  \n",
      " 7   textlemma           16902 non-null  object \n",
      " 8   textlengthforlemma  16902 non-null  int64  \n",
      " 9   word_count          16902 non-null  int64  \n",
      " 10  topic               16902 non-null  int64  \n",
      " 11  topic_name          16902 non-null  object \n",
      " 12  year                16902 non-null  int64  \n",
      " 13  count               16902 non-null  int64  \n",
      "dtypes: float64(1), int64(7), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/pedige1/.local/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:59:03.563765: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 10:59:03.915440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745751544.036592  102830 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745751544.073883  102830 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745751544.371819  102830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751544.371866  102830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751544.371873  102830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751544.371877  102830 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 10:59:04.408150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "/home/pedige1/.local/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load NER\n",
    "#import transformers\n",
    "#from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "#from transformers import  DistilBertForTokenClassification, DistilBertTokenizer, DistilBertModel\n",
    "#tokenizer_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "#model_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "#nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, grouped_entities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "def generate_entities(news):\n",
    "  results = nlp(news)\n",
    "  entities = [res['word'] for res in results]\n",
    "  return entities\n",
    "\n",
    "data['Entities'] = data['textlemma'].apply(generate_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \\\n",
       "0                 138          20      7  telecom industry  2010    472   \n",
       "1                  34           6      2             stock  2010    578   \n",
       "2                  78          11      0          pharmacy  2010    673   \n",
       "3                  80          13      0          pharmacy  2010    673   \n",
       "4                 137          21      7  telecom industry  2010    472   \n",
       "\n",
       "  Entities  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]                        13680\n",
       "[##eric]                     64\n",
       "[euro]                       57\n",
       "[american]                   46\n",
       "[chinese]                    43\n",
       "                          ...  \n",
       "[european, g, ##erman]        1\n",
       "[eio adamis]                  1\n",
       "[cenzic, opso, ##ce]          1\n",
       "[armani]                      1\n",
       "[##var, european]             1\n",
       "Name: Entities, Length: 1779, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Entities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in /home/pedige1/.local/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/lib/python3/dist-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from sentence_transformers) (1.8.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from sentence_transformers) (0.23.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence_transformers) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: tqdm in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in /home/pedige1/.local/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /home/pedige1/.local/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (59.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Collecting numpy<2.2.0,>=1.26.0\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.13.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (21.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.5)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.8.3 requires numpy<2,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Article_1_Text  \\\n",
      "4    intercept pharmaceutical report second quarter...   \n",
      "120  intercept pharmaceutical report second quarter...   \n",
      "296  research and market cancer diagnostics market ...   \n",
      "322  research and market cancer diagnostics market ...   \n",
      "456  research and market pharma blogging speaking o...   \n",
      "\n",
      "                                        Article_2_Text  Date_Difference  \\\n",
      "4    addex therapeutic first half financial result ...              721   \n",
      "120       ligand report first quarter financial result              460   \n",
      "296  research and market the global pharmaceutical ...              706   \n",
      "322  research and market future molecular diagnosti...              427   \n",
      "456  panel top industry executive discus biotech an...              335   \n",
      "\n",
      "     SBERT_Similarity Entities_1 Entities_2  \n",
      "4            0.575474         []         []  \n",
      "120          0.525239         []         []  \n",
      "296          0.500552         []         []  \n",
      "322          0.689295         []         []  \n",
      "456          0.557010         []         []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MPNet-base-v2')\n",
    "\n",
    "filtered_articles = data[['textlemma', 'date', 'Entities']]\n",
    "filtered_articles = filtered_articles.dropna(subset=['textlemma', 'date', 'Entities'])\n",
    "filtered_articles = filtered_articles.sample(200, random_state=42)\n",
    "filtered_articles['date'] = pd.to_datetime(filtered_articles['date'])\n",
    "\n",
    "#calculate SBERT similarity between articles\n",
    "def calculate_sbert_similarity(text1, text2):\n",
    "    if not text1 or not text2:\n",
    "        return 0\n",
    "\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity\n",
    "\n",
    "# Compare similarities between articles\n",
    "similarities = []\n",
    "pairs = list(combinations(filtered_articles.index, 2))\n",
    "\n",
    "for i, j in pairs:\n",
    "    text1 = filtered_articles.loc[i, \"textlemma\"]\n",
    "    text2 = filtered_articles.loc[j, \"textlemma\"]\n",
    "\n",
    "    sim = calculate_sbert_similarity(text1, text2)\n",
    "    date_difference = abs((filtered_articles.loc[i, \"date\"] - filtered_articles.loc[j, \"date\"]).days)\n",
    "\n",
    "    similarities.append({\n",
    "        \"Article_1_Text\": text1,\n",
    "        \"Article_2_Text\": text2,\n",
    "        \"Date_Difference\": date_difference,\n",
    "        \"SBERT_Similarity\": sim,\n",
    "        \"Entities_1\": filtered_articles.loc[i, \"Entities\"],\n",
    "        \"Entities_2\": filtered_articles.loc[j, \"Entities\"]\n",
    "    })\n",
    "\n",
    "# Turn into DataFrame and filter high similarity\n",
    "similarity_df = pd.DataFrame(similarities)\n",
    "high_similarity = similarity_df.query('SBERT_Similarity > 0.5')\n",
    "print(high_similarity.head())\n",
    "\n",
    "# Save the results to a CSV file\n",
    "similarity_df.to_csv('similarity_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_articles['word_count'] = filtered_articles['textlemma'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    200.000000\n",
      "mean       8.940000\n",
      "std        3.634895\n",
      "min        3.000000\n",
      "25%        6.000000\n",
      "50%        8.000000\n",
      "75%       11.000000\n",
      "max       20.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_articles['word_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum word count: 3\n",
      "Maximum word count: 20\n"
     ]
    }
   ],
   "source": [
    "min_words = filtered_articles['word_count'].min()\n",
    "max_words = filtered_articles['word_count'].max()\n",
    "\n",
    "print(f\"Minimum word count: {min_words}\")\n",
    "print(f\"Maximum word count: {max_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches from 5001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MPNet-base-v2')\n",
    "\n",
    "# Prepare your dataset\n",
    "filtered_articles = data[['textlemma', 'date', 'Entities']]\n",
    "filtered_articles = filtered_articles.dropna(subset=['textlemma', 'date', 'Entities'])\n",
    "filtered_articles['date'] = pd.to_datetime(filtered_articles['date'])\n",
    "\n",
    "# Select the batch: 1000 articles starting from 5001st\n",
    "batch = filtered_articles.iloc[5000:6000]\n",
    "\n",
    "# Encode all articles in batch at once\n",
    "embeddings = model.encode(batch['textlemma'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute full pairwise cosine similarity matrix\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Get only upper triangle indices (no duplicate comparisons)\n",
    "indices = torch.triu_indices(cosine_scores.size(0), cosine_scores.size(1), offset=1)\n",
    "\n",
    "i_indices, j_indices = indices[0], indices[1]\n",
    "similarity_values = cosine_scores[i_indices, j_indices]\n",
    "\n",
    "# Build the results DataFrame\n",
    "similarity_df = pd.DataFrame({\n",
    "    \"Article_1_Text\": batch.iloc[i_indices][\"textlemma\"].values,\n",
    "    \"Article_2_Text\": batch.iloc[j_indices][\"textlemma\"].values,\n",
    "    \"SBERT_Similarity\": similarity_values.cpu().numpy(),\n",
    "    \"Entities_1\": batch.iloc[i_indices][\"Entities\"].values,\n",
    "    \"Entities_2\": batch.iloc[j_indices][\"Entities\"].values,\n",
    "    \"Date_Difference\": abs(\n",
    "        (batch.iloc[i_indices][\"date\"].values - batch.iloc[j_indices][\"date\"].values)\n",
    "    ).astype('timedelta64[D]').astype(int)\n",
    "})\n",
    "\n",
    "# Save the full similarity results to a CSV file\n",
    "similarity_df.to_csv('1000_similarity_articles_from_5001.csv', index=False)\n",
    "\n",
    "print(\" Similarity computation complete and saved to '1000_similarity_articles_from_5001.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All batches at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 to 1000...\n",
      "Saved batch 1 to 1000 at batch_similarity_results/similarity_batch_1_to_1000.csv\n",
      "Processing batch 1001 to 2000...\n",
      "Saved batch 1001 to 2000 at batch_similarity_results/similarity_batch_1001_to_2000.csv\n",
      "Processing batch 2001 to 3000...\n",
      "Saved batch 2001 to 3000 at batch_similarity_results/similarity_batch_2001_to_3000.csv\n",
      "Processing batch 3001 to 4000...\n",
      "Saved batch 3001 to 4000 at batch_similarity_results/similarity_batch_3001_to_4000.csv\n",
      "Processing batch 4001 to 5000...\n",
      "Saved batch 4001 to 5000 at batch_similarity_results/similarity_batch_4001_to_5000.csv\n",
      "Processing batch 5001 to 6000...\n",
      "Saved batch 5001 to 6000 at batch_similarity_results/similarity_batch_5001_to_6000.csv\n",
      "Processing batch 6001 to 7000...\n",
      "Saved batch 6001 to 7000 at batch_similarity_results/similarity_batch_6001_to_7000.csv\n",
      "Processing batch 7001 to 8000...\n",
      "Saved batch 7001 to 8000 at batch_similarity_results/similarity_batch_7001_to_8000.csv\n",
      "Processing batch 8001 to 9000...\n",
      "Saved batch 8001 to 9000 at batch_similarity_results/similarity_batch_8001_to_9000.csv\n",
      "Processing batch 9001 to 10000...\n",
      "Saved batch 9001 to 10000 at batch_similarity_results/similarity_batch_9001_to_10000.csv\n",
      "Processing batch 10001 to 11000...\n",
      "Saved batch 10001 to 11000 at batch_similarity_results/similarity_batch_10001_to_11000.csv\n",
      "Processing batch 11001 to 12000...\n",
      "Saved batch 11001 to 12000 at batch_similarity_results/similarity_batch_11001_to_12000.csv\n",
      "Processing batch 12001 to 13000...\n",
      "Saved batch 12001 to 13000 at batch_similarity_results/similarity_batch_12001_to_13000.csv\n",
      "Processing batch 13001 to 14000...\n",
      "Saved batch 13001 to 14000 at batch_similarity_results/similarity_batch_13001_to_14000.csv\n",
      "Processing batch 14001 to 15000...\n",
      "Saved batch 14001 to 15000 at batch_similarity_results/similarity_batch_14001_to_15000.csv\n",
      "Processing batch 15001 to 16000...\n",
      "Saved batch 15001 to 16000 at batch_similarity_results/similarity_batch_15001_to_16000.csv\n",
      "Processing batch 16001 to 16902...\n",
      "Saved batch 16001 to 16902 at batch_similarity_results/similarity_batch_16001_to_16902.csv\n",
      " All batches processed and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MPNet-base-v2')\n",
    "\n",
    "# Load and prepare dataset\n",
    "filtered_articles = data[['textlemma', 'date', 'Entities']]\n",
    "filtered_articles = filtered_articles.dropna(subset=['textlemma', 'date', 'Entities'])\n",
    "filtered_articles['date'] = pd.to_datetime(filtered_articles['date'])\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1000\n",
    "output_folder = 'batch_similarity_results'  \n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the dataset in batches\n",
    "for start_idx in range(0, len(filtered_articles), batch_size):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = filtered_articles.iloc[start_idx:end_idx]\n",
    "\n",
    "    if len(batch) < 2:\n",
    "        print(f\"Skipping batch {start_idx}-{end_idx}: Not enough articles.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing batch {start_idx + 1} to {min(end_idx, len(filtered_articles))}...\")\n",
    "\n",
    "    # Encode all articles in batch\n",
    "    embeddings = model.encode(batch['textlemma'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Get upper triangle indices\n",
    "    indices = torch.triu_indices(cosine_scores.size(0), cosine_scores.size(1), offset=1)\n",
    "    i_indices, j_indices = indices[0], indices[1]\n",
    "    similarity_values = cosine_scores[i_indices, j_indices]\n",
    "\n",
    "    # Build results DataFrame\n",
    "    similarity_df = pd.DataFrame({\n",
    "        \"Article_1_Text\": batch.iloc[i_indices][\"textlemma\"].values,\n",
    "        \"Article_2_Text\": batch.iloc[j_indices][\"textlemma\"].values,\n",
    "        \"SBERT_Similarity\": similarity_values.cpu().numpy(),\n",
    "        \"Entities_1\": batch.iloc[i_indices][\"Entities\"].values,\n",
    "        \"Entities_2\": batch.iloc[j_indices][\"Entities\"].values,\n",
    "        \"Date_Difference\": abs(\n",
    "            (batch.iloc[i_indices][\"date\"].values - batch.iloc[j_indices][\"date\"].values)\n",
    "        ).astype('timedelta64[D]').astype(int)\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    batch_filename = f\"similarity_batch_{start_idx+1}_to_{min(end_idx, len(filtered_articles))}.csv\"\n",
    "    batch_path = os.path.join(output_folder, batch_filename)\n",
    "    similarity_df.to_csv(batch_path, index=False)\n",
    "\n",
    "    print(f\"Saved batch {start_idx + 1} to {min(end_idx, len(filtered_articles))} at {batch_path}\")\n",
    "\n",
    "print(\" All batches processed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading batch_similarity_results/similarity_batch_8001_to_9000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_14001_to_15000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_2001_to_3000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_6001_to_7000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_16001_to_16902.csv...\n",
      " Loading batch_similarity_results/similarity_batch_15001_to_16000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_9001_to_10000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_10001_to_11000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_1_to_1000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_13001_to_14000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_11001_to_12000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_12001_to_13000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_1001_to_2000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_4001_to_5000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_5001_to_6000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_3001_to_4000.csv...\n",
      " Loading batch_similarity_results/similarity_batch_7001_to_8000.csv...\n",
      "Merged 17 batch files into 'full_similarity_merged.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder where batch results are saved\n",
    "batch_folder = 'batch_similarity_results'\n",
    "\n",
    "# Pattern to match all batch CSV files\n",
    "batch_files = glob.glob(os.path.join(batch_folder, 'similarity_batch_*.csv'))\n",
    "\n",
    "# List to store all dataframes\n",
    "all_batches = []\n",
    "\n",
    "# Read each batch file and store\n",
    "for file in batch_files:\n",
    "    print(f\" Loading {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    all_batches.append(df)\n",
    "\n",
    "# Concatenate all batches into one DataFrame\n",
    "full_similarity_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Save the full merged file\n",
    "full_similarity_df.to_csv('full_similarity_merged.csv', index=False)\n",
    "\n",
    "print(f\"Merged {len(batch_files)} batch files into 'full_similarity_merged.csv'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \n",
       "0                 138          20      7  telecom industry  2010    472  \n",
       "1                  34           6      2             stock  2010    578  \n",
       "2                  78          11      0          pharmacy  2010    673  \n",
       "3                  80          13      0          pharmacy  2010    673  \n",
       "4                 137          21      7  telecom industry  2010    472  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapre = pd.read_csv (\"/home/pedige1/DQ-2/train_data.csv\")\n",
    "datapre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest headline has: 225 words\n"
     ]
    }
   ],
   "source": [
    "print(\"The longest headline has: {} words\".format(datapre.text_length.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    16902.000000\n",
      "mean         9.163176\n",
      "std          3.984868\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         11.000000\n",
      "max         28.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_stats = datapre['word_count'].describe()\n",
    "print(word_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapre['lowercasedtext'] = datapre['news'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "      <th>lowercasedtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>research and markets: the history of wireless:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "      <td>u.s. stock futures signal gains; eyes on alcoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>research and markets: epilepsy - drug pipeline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>weird science?  big pharma's top three m&amp;a; d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>research and markets: 2009 state of the china ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \\\n",
       "0                 138          20      7  telecom industry  2010    472   \n",
       "1                  34           6      2             stock  2010    578   \n",
       "2                  78          11      0          pharmacy  2010    673   \n",
       "3                  80          13      0          pharmacy  2010    673   \n",
       "4                 137          21      7  telecom industry  2010    472   \n",
       "\n",
       "                                      lowercasedtext  \n",
       "0  research and markets: the history of wireless:...  \n",
       "1     u.s. stock futures signal gains; eyes on alcoa  \n",
       "2  research and markets: epilepsy - drug pipeline...  \n",
       "3   weird science?  big pharma's top three m&a; d...  \n",
       "4  research and markets: 2009 state of the china ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 to 1000...\n",
      "Saved batch 1 to 1000 at batch_similarity_results_lowercased/similarity_batch_1_to_1000.csv\n",
      "Processing batch 1001 to 2000...\n",
      "Saved batch 1001 to 2000 at batch_similarity_results_lowercased/similarity_batch_1001_to_2000.csv\n",
      "Processing batch 2001 to 3000...\n",
      "Saved batch 2001 to 3000 at batch_similarity_results_lowercased/similarity_batch_2001_to_3000.csv\n",
      "Processing batch 3001 to 4000...\n",
      "Saved batch 3001 to 4000 at batch_similarity_results_lowercased/similarity_batch_3001_to_4000.csv\n",
      "Processing batch 4001 to 5000...\n",
      "Saved batch 4001 to 5000 at batch_similarity_results_lowercased/similarity_batch_4001_to_5000.csv\n",
      "Processing batch 5001 to 6000...\n",
      "Saved batch 5001 to 6000 at batch_similarity_results_lowercased/similarity_batch_5001_to_6000.csv\n",
      "Processing batch 6001 to 7000...\n",
      "Saved batch 6001 to 7000 at batch_similarity_results_lowercased/similarity_batch_6001_to_7000.csv\n",
      "Processing batch 7001 to 8000...\n",
      "Saved batch 7001 to 8000 at batch_similarity_results_lowercased/similarity_batch_7001_to_8000.csv\n",
      "Processing batch 8001 to 9000...\n",
      "Saved batch 8001 to 9000 at batch_similarity_results_lowercased/similarity_batch_8001_to_9000.csv\n",
      "Processing batch 9001 to 10000...\n",
      "Saved batch 9001 to 10000 at batch_similarity_results_lowercased/similarity_batch_9001_to_10000.csv\n",
      "Processing batch 10001 to 11000...\n",
      "Saved batch 10001 to 11000 at batch_similarity_results_lowercased/similarity_batch_10001_to_11000.csv\n",
      "Processing batch 11001 to 12000...\n",
      "Saved batch 11001 to 12000 at batch_similarity_results_lowercased/similarity_batch_11001_to_12000.csv\n",
      "Processing batch 12001 to 13000...\n",
      "Saved batch 12001 to 13000 at batch_similarity_results_lowercased/similarity_batch_12001_to_13000.csv\n",
      "Processing batch 13001 to 14000...\n",
      "Saved batch 13001 to 14000 at batch_similarity_results_lowercased/similarity_batch_13001_to_14000.csv\n",
      "Processing batch 14001 to 15000...\n",
      "Saved batch 14001 to 15000 at batch_similarity_results_lowercased/similarity_batch_14001_to_15000.csv\n",
      "Processing batch 15001 to 16000...\n",
      "Saved batch 15001 to 16000 at batch_similarity_results_lowercased/similarity_batch_15001_to_16000.csv\n",
      "Processing batch 16001 to 16902...\n",
      "Saved batch 16001 to 16902 at batch_similarity_results_lowercased/similarity_batch_16001_to_16902.csv\n",
      " All batches processed and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MPNet-base-v2')\n",
    "\n",
    "# Load and prepare dataset\n",
    "filtered_articles = datapre[['lowercasedtext', 'date', ]]\n",
    "filtered_articles = filtered_articles.dropna(subset=['lowercasedtext', 'date'])\n",
    "filtered_articles['date'] = pd.to_datetime(filtered_articles['date'])\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1000\n",
    "output_folder = 'batch_similarity_results_lowercased'  # Folder for lowercased results\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the dataset in batches\n",
    "for start_idx in range(0, len(filtered_articles), batch_size):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = filtered_articles.iloc[start_idx:end_idx]\n",
    "\n",
    "    if len(batch) < 2:\n",
    "        print(f\"Skipping batch {start_idx}-{end_idx}: Not enough articles.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing batch {start_idx + 1} to {min(end_idx, len(filtered_articles))}...\")\n",
    "\n",
    "    # Encode all articles in batch\n",
    "    embeddings = model.encode(batch['lowercasedtext'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Get upper triangle indices\n",
    "    indices = torch.triu_indices(cosine_scores.size(0), cosine_scores.size(1), offset=1)\n",
    "    i_indices, j_indices = indices[0], indices[1]\n",
    "    similarity_values = cosine_scores[i_indices, j_indices]\n",
    "\n",
    "    # Build results DataFrame\n",
    "    similarity_df_lowercased = pd.DataFrame({\n",
    "        \"LowerC_Article_1_Text\": batch.iloc[i_indices][\"lowercasedtext\"].values,\n",
    "        \"LowerC_Article_2_Text\": batch.iloc[j_indices][\"lowercasedtext\"].values,\n",
    "        \"LowerC_SBERT_Similarity\": similarity_values.cpu().numpy(),\n",
    "        #\"LowerC_Entities_1\": batch.iloc[i_indices][\"Entities\"].values,\n",
    "        #\"LowerC_Entities_2\": batch.iloc[j_indices][\"Entities\"].values,\n",
    "        \"LowerC_Date_Difference\": abs(\n",
    "            (batch.iloc[i_indices][\"date\"].values - batch.iloc[j_indices][\"date\"].values)\n",
    "        ).astype('timedelta64[D]').astype(int)\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    batch_filename = f\"similarity_batch_{start_idx+1}_to_{min(end_idx, len(filtered_articles))}.csv\"\n",
    "    batch_path = os.path.join(output_folder, batch_filename)\n",
    "    similarity_df_lowercased.to_csv(batch_path, index=False)\n",
    "\n",
    "    print(f\"Saved batch {start_idx + 1} to {min(end_idx, len(filtered_articles))} at {batch_path}\")\n",
    "\n",
    "print(\" All batches processed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading batch_similarity_results_lowercased/similarity_batch_8001_to_9000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_14001_to_15000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_2001_to_3000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_6001_to_7000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_16001_to_16902.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_15001_to_16000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_9001_to_10000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_10001_to_11000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_1_to_1000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_13001_to_14000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_11001_to_12000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_12001_to_13000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_1001_to_2000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_4001_to_5000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_5001_to_6000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_3001_to_4000.csv...\n",
      " Loading batch_similarity_results_lowercased/similarity_batch_7001_to_8000.csv...\n",
      "Merged 17 batch files into 'lowercased_full_similarity_merged.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder where batch results are saved\n",
    "batch_folder = 'batch_similarity_results_lowercased'\n",
    "\n",
    "# Pattern to match all batch CSV files\n",
    "batch_files = glob.glob(os.path.join(batch_folder, 'similarity_batch_*.csv'))\n",
    "\n",
    "# List to store all dataframes\n",
    "all_batches = []\n",
    "\n",
    "# Read each batch file and store\n",
    "for file in batch_files:\n",
    "    print(f\" Loading {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    all_batches.append(df)\n",
    "\n",
    "# Concatenate all batches into one DataFrame\n",
    "lowercased_full_similarity_df = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Save the full merged file\n",
    "lowercased_full_similarity_df.to_csv('lowercased_full_similarity_merged.csv', index=False)\n",
    "\n",
    "print(f\"Merged {len(batch_files)} batch files into 'lowercased_full_similarity_merged.csv'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging data form the Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load both CSVs\n",
    "Similarity_df_1 = pd.read_csv('/home/pedige1/DQ-3/full_similarity_merged.csv')\n",
    "Lsimilarity_df_2 = pd.read_csv('/home/pedige1/DQ-3/lowercased_full_similarity_merged.csv')\n",
    "\n",
    "# Step 2: Select only the needed columns from the second dataset\n",
    "columns_to_add = ['LowerC_Article_1_Text', 'LowerC_Article_2_Text', \n",
    "                  'LowerC_SBERT_Similarity', 'LowerC_Date_Difference']\n",
    "lowercased_subset = Lsimilarity_df_2[columns_to_add]\n",
    "\n",
    "# Step 3: Merge the two DataFrames (assumes they are aligned by index)\n",
    "merged_df = pd.concat([Similarity_df_1, lowercased_subset], axis=1)\n",
    "\n",
    "# Step 4: Save to a new CSV file\n",
    "merged_df.to_csv('merged_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_1_Text</th>\n",
       "      <th>Article_2_Text</th>\n",
       "      <th>SBERT_Similarity</th>\n",
       "      <th>Entities_1</th>\n",
       "      <th>Entities_2</th>\n",
       "      <th>Date_Difference</th>\n",
       "      <th>LowerC_Article_1_Text</th>\n",
       "      <th>LowerC_Article_2_Text</th>\n",
       "      <th>LowerC_SBERT_Similarity</th>\n",
       "      <th>LowerC_Date_Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stock snapshot future briefly turn negative af...</td>\n",
       "      <td>stock snapshot future turn positive after adp ...</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>us stocks snapshot - futures briefly turn nega...</td>\n",
       "      <td>us stocks snapshot - futures turn positive aft...</td>\n",
       "      <td>0.691441</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stock snapshot future briefly turn negative af...</td>\n",
       "      <td>trustwave security expert present application ...</td>\n",
       "      <td>0.056198</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>us stocks snapshot - futures briefly turn nega...</td>\n",
       "      <td>trustwave security experts to present at appl...</td>\n",
       "      <td>0.038896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stock snapshot future briefly turn negative af...</td>\n",
       "      <td>stock future dip wake data central bank move</td>\n",
       "      <td>0.511406</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>us stocks snapshot - futures briefly turn nega...</td>\n",
       "      <td>us stocks-futures dip in wake of data, central...</td>\n",
       "      <td>0.709325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stock snapshot future briefly turn negative af...</td>\n",
       "      <td>stock wall set for lower open after central ba...</td>\n",
       "      <td>0.424959</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>us stocks snapshot - futures briefly turn nega...</td>\n",
       "      <td>us stocks-wall st set for lower open after cen...</td>\n",
       "      <td>0.503095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stock snapshot future briefly turn negative af...</td>\n",
       "      <td>research and market epilepsy partnering</td>\n",
       "      <td>0.099519</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>us stocks snapshot - futures briefly turn nega...</td>\n",
       "      <td>research and markets: epilepsy partnering 2007...</td>\n",
       "      <td>0.128975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Article_1_Text  \\\n",
       "0  stock snapshot future briefly turn negative af...   \n",
       "1  stock snapshot future briefly turn negative af...   \n",
       "2  stock snapshot future briefly turn negative af...   \n",
       "3  stock snapshot future briefly turn negative af...   \n",
       "4  stock snapshot future briefly turn negative af...   \n",
       "\n",
       "                                      Article_2_Text  SBERT_Similarity  \\\n",
       "0  stock snapshot future turn positive after adp ...          0.706587   \n",
       "1  trustwave security expert present application ...          0.056198   \n",
       "2       stock future dip wake data central bank move          0.511406   \n",
       "3  stock wall set for lower open after central ba...          0.424959   \n",
       "4            research and market epilepsy partnering          0.099519   \n",
       "\n",
       "  Entities_1 Entities_2  Date_Difference  \\\n",
       "0         []         []                0   \n",
       "1         []         []                0   \n",
       "2         []         []                0   \n",
       "3         []         []                0   \n",
       "4         []         []                0   \n",
       "\n",
       "                               LowerC_Article_1_Text  \\\n",
       "0  us stocks snapshot - futures briefly turn nega...   \n",
       "1  us stocks snapshot - futures briefly turn nega...   \n",
       "2  us stocks snapshot - futures briefly turn nega...   \n",
       "3  us stocks snapshot - futures briefly turn nega...   \n",
       "4  us stocks snapshot - futures briefly turn nega...   \n",
       "\n",
       "                               LowerC_Article_2_Text  LowerC_SBERT_Similarity  \\\n",
       "0  us stocks snapshot - futures turn positive aft...                 0.691441   \n",
       "1   trustwave security experts to present at appl...                 0.038896   \n",
       "2  us stocks-futures dip in wake of data, central...                 0.709325   \n",
       "3  us stocks-wall st set for lower open after cen...                 0.503095   \n",
       "4  research and markets: epilepsy partnering 2007...                 0.128975   \n",
       "\n",
       "   LowerC_Date_Difference  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8398351 entries, 0 to 8398350\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   Article_1_Text           object \n",
      " 1   Article_2_Text           object \n",
      " 2   SBERT_Similarity         float64\n",
      " 3   Entities_1               object \n",
      " 4   Entities_2               object \n",
      " 5   Date_Difference          int64  \n",
      " 6   LowerC_Article_1_Text    object \n",
      " 7   LowerC_Article_2_Text    object \n",
      " 8   LowerC_SBERT_Similarity  float64\n",
      " 9   LowerC_Date_Difference   int64  \n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 640.7+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
