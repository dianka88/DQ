{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.21.5\n",
      "Not uninstalling numpy at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "Found existing installation: pandas 1.3.5\n",
      "Not uninstalling pandas at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "Can't uninstall 'pandas'. No files were found to uninstall.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install --no-cache-dir numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \n",
       "0                 138          20      7  telecom industry  2010    472  \n",
       "1                  34           6      2             stock  2010    578  \n",
       "2                  78          11      0          pharmacy  2010    673  \n",
       "3                  80          13      0          pharmacy  2010    673  \n",
       "4                 137          21      7  telecom industry  2010    472  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv (\"/home/pedige1/DQ-2/train_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            16902\n",
       "id                    16902\n",
       "date                  16902\n",
       "news                  16902\n",
       "label                 16902\n",
       "pct_change            16902\n",
       "text_length           16902\n",
       "textlemma             16902\n",
       "textlengthforlemma    16902\n",
       "word_count            16902\n",
       "topic                 16902\n",
       "topic_name            16902\n",
       "year                  16902\n",
       "count                 16902\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16902 entries, 0 to 16901\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          16902 non-null  int64  \n",
      " 1   id                  16902 non-null  object \n",
      " 2   date                16902 non-null  object \n",
      " 3   news                16902 non-null  object \n",
      " 4   label               16902 non-null  object \n",
      " 5   pct_change          16902 non-null  float64\n",
      " 6   text_length         16902 non-null  int64  \n",
      " 7   textlemma           16902 non-null  object \n",
      " 8   textlengthforlemma  16902 non-null  int64  \n",
      " 9   word_count          16902 non-null  int64  \n",
      " 10  topic               16902 non-null  int64  \n",
      " 11  topic_name          16902 non-null  object \n",
      " 12  year                16902 non-null  int64  \n",
      " 13  count               16902 non-null  int64  \n",
      "dtypes: float64(1), int64(7), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/pedige1/.local/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "/home/pedige1/.local/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load NER\n",
    "#import transformers\n",
    "#from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "#from transformers import  DistilBertForTokenClassification, DistilBertTokenizer, DistilBertModel\n",
    "#tokenizer_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "#model_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "#nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, grouped_entities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entities(news):\n",
    "  results = nlp(news)\n",
    "  entities = [res['word'] for res in results]\n",
    "  return entities\n",
    "\n",
    "data['Entities'] = data['textlemma'].apply(generate_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>text_length</th>\n",
       "      <th>textlemma</th>\n",
       "      <th>textlengthforlemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: The History of Wireless:...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>148</td>\n",
       "      <td>research and market the history wireless how c...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>U.S. stock futures signal gains; eyes on Alcoa</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>46</td>\n",
       "      <td>stock future signal gain eye alcoa</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>stock</td>\n",
       "      <td>2010</td>\n",
       "      <td>578</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: Epilepsy - Drug Pipeline...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>91</td>\n",
       "      <td>research and market epilepsy drug pipeline ana...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Weird Science?  Big Pharma's Top Three M&amp;A; D...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>108</td>\n",
       "      <td>weird science big pharma top three deal nears ...</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>2010</td>\n",
       "      <td>673</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>nifty_2</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>Research and Markets: 2009 State of the China ...</td>\n",
       "      <td>Fall</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>177</td>\n",
       "      <td>research and market state the china smb market...</td>\n",
       "      <td>137</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>telecom industry</td>\n",
       "      <td>2010</td>\n",
       "      <td>472</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        date  \\\n",
       "0          71  nifty_2  2010-01-11   \n",
       "1          97  nifty_2  2010-01-11   \n",
       "2         104  nifty_2  2010-01-11   \n",
       "3         127  nifty_2  2010-01-11   \n",
       "4         174  nifty_2  2010-01-11   \n",
       "\n",
       "                                                news label  pct_change  \\\n",
       "0  Research and Markets: The History of Wireless:...  Fall     -0.0093   \n",
       "1     U.S. stock futures signal gains; eyes on Alcoa  Fall     -0.0093   \n",
       "2  Research and Markets: Epilepsy - Drug Pipeline...  Fall     -0.0093   \n",
       "3   Weird Science?  Big Pharma's Top Three M&A; D...  Fall     -0.0093   \n",
       "4  Research and Markets: 2009 State of the China ...  Fall     -0.0093   \n",
       "\n",
       "   text_length                                          textlemma  \\\n",
       "0          148  research and market the history wireless how c...   \n",
       "1           46                 stock future signal gain eye alcoa   \n",
       "2           91  research and market epilepsy drug pipeline ana...   \n",
       "3          108  weird science big pharma top three deal nears ...   \n",
       "4          177  research and market state the china smb market...   \n",
       "\n",
       "   textlengthforlemma  word_count  topic        topic_name  year  count  \\\n",
       "0                 138          20      7  telecom industry  2010    472   \n",
       "1                  34           6      2             stock  2010    578   \n",
       "2                  78          11      0          pharmacy  2010    673   \n",
       "3                  80          13      0          pharmacy  2010    673   \n",
       "4                 137          21      7  telecom industry  2010    472   \n",
       "\n",
       "  Entities  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]                        13680\n",
       "[##eric]                     64\n",
       "[euro]                       57\n",
       "[american]                   46\n",
       "[chinese]                    43\n",
       "                          ...  \n",
       "[european, g, ##erman]        1\n",
       "[eio adamis]                  1\n",
       "[cenzic, opso, ##ce]          1\n",
       "[armani]                      1\n",
       "[##var, european]             1\n",
       "Name: Entities, Length: 1779, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Entities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in /home/pedige1/.local/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/lib/python3/dist-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence_transformers) (9.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from sentence_transformers) (0.23.2)\n",
      "Requirement already satisfied: tqdm in /home/pedige1/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from sentence_transformers) (1.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pedige1/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in /home/pedige1/.local/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /home/pedige1/.local/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (21.3)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.13.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (59.6.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/pedige1/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Article_1_Text  \\\n",
      "4    intercept pharmaceutical report second quarter...   \n",
      "120  intercept pharmaceutical report second quarter...   \n",
      "296  research and market cancer diagnostics market ...   \n",
      "322  research and market cancer diagnostics market ...   \n",
      "456  research and market pharma blogging speaking o...   \n",
      "\n",
      "                                        Article_2_Text  Date_Difference  \\\n",
      "4    addex therapeutic first half financial result ...              721   \n",
      "120       ligand report first quarter financial result              460   \n",
      "296  research and market the global pharmaceutical ...              706   \n",
      "322  research and market future molecular diagnosti...              427   \n",
      "456  panel top industry executive discus biotech an...              335   \n",
      "\n",
      "     SBERT_Similarity Entities_1 Entities_2  \n",
      "4            0.575474         []         []  \n",
      "120          0.525239         []         []  \n",
      "296          0.500552         []         []  \n",
      "322          0.689295         []         []  \n",
      "456          0.557010         []         []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MPNet-base-v2')\n",
    "\n",
    "filtered_articles = data[['textlemma', 'date', 'Entities']]\n",
    "filtered_articles = filtered_articles.dropna(subset=['textlemma', 'date', 'Entities'])\n",
    "filtered_articles = filtered_articles.sample(200, random_state=42)\n",
    "filtered_articles['date'] = pd.to_datetime(filtered_articles['date'])\n",
    "\n",
    "#calculate SBERT similarity between articles\n",
    "def calculate_sbert_similarity(text1, text2):\n",
    "    if not text1 or not text2:\n",
    "        return 0\n",
    "\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity\n",
    "\n",
    "# Compare similarities between articles\n",
    "similarities = []\n",
    "pairs = list(combinations(filtered_articles.index, 2))\n",
    "\n",
    "for i, j in pairs:\n",
    "    text1 = filtered_articles.loc[i, \"textlemma\"]\n",
    "    text2 = filtered_articles.loc[j, \"textlemma\"]\n",
    "\n",
    "    sim = calculate_sbert_similarity(text1, text2)\n",
    "    date_difference = abs((filtered_articles.loc[i, \"date\"] - filtered_articles.loc[j, \"date\"]).days)\n",
    "\n",
    "    similarities.append({\n",
    "        \"Article_1_Text\": text1,\n",
    "        \"Article_2_Text\": text2,\n",
    "        \"Date_Difference\": date_difference,\n",
    "        \"SBERT_Similarity\": sim,\n",
    "        \"Entities_1\": filtered_articles.loc[i, \"Entities\"],\n",
    "        \"Entities_2\": filtered_articles.loc[j, \"Entities\"]\n",
    "    })\n",
    "\n",
    "# Turn into DataFrame and filter high similarity\n",
    "similarity_df = pd.DataFrame(similarities)\n",
    "high_similarity = similarity_df.query('SBERT_Similarity > 0.5')\n",
    "print(high_similarity.head())\n",
    "\n",
    "# Save the results to a CSV file\n",
    "similarity_df.to_csv('similarity_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_articles['word_count'] = filtered_articles['textlemma'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    200.000000\n",
      "mean       8.940000\n",
      "std        3.634895\n",
      "min        3.000000\n",
      "25%        6.000000\n",
      "50%        8.000000\n",
      "75%       11.000000\n",
      "max       20.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_articles['word_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum word count: 3\n",
      "Maximum word count: 20\n"
     ]
    }
   ],
   "source": [
    "min_words = filtered_articles['word_count'].min()\n",
    "max_words = filtered_articles['word_count'].max()\n",
    "\n",
    "print(f\"Minimum word count: {min_words}\")\n",
    "print(f\"Maximum word count: {max_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
